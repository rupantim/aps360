{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO4OksS4R1U9"
   },
   "source": [
    "# Tutorial - Generative Recurrent Neural Networks\n",
    "\n",
    "Last time we discussed using recurrent neural networks to make predictions about sequences. In particular, we treated tweets as a **sequence** of words. Since tweets can have a variable number of words, we needed an architecture that can take variable-sized sequences as input.\n",
    "\n",
    "This time, we will use recurrent neural networks to **generate** sequences.\n",
    "Generating sequences is more involved comparing to making predictions about\n",
    "sequences. However, it is a very interesting task, and many students chose\n",
    "sequence-generation tasks for their projects.\n",
    "\n",
    "Much of today's content is an adaptation of the \"Practical PyTorch\" github \n",
    "repository [1].\n",
    "\n",
    "[1] https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiwPm7atR1VC"
   },
   "source": [
    "## Review\n",
    "\n",
    "In recurrent neural networks the input sequence is broken down into tokens. We could choose whether to tokenize based on words, or based on characters. The representation of each token (GloVe or one-hot) is processed by the RNN one step at a time to update the hidden (or context) state.\n",
    "\n",
    "In a predictive RNN, the value of the hidden states  is a representation of **all the text that was processed thus far**. Similarly, in a generative RNN, The value of the hidden state will be a representation of **all the text that still needs to be generated**. We will use this hidden state to produce the sequence, one token at a time.\n",
    "\n",
    "Similar to the last tutorial we will break up the problem of generating text\n",
    "to generating one token at a time.\n",
    "\n",
    "We will do so with the help of two functions:\n",
    "\n",
    "1. We need to be able to generate the *next* token, given the current \n",
    "   hidden state. In practice, we get a probability distribution over \n",
    "   the next token, and sample from that probability distribution.\n",
    "2. We need to be able to update the hidden state somehow. To do so,\n",
    "   we need two piece of information: the old hidden state, and the actual\n",
    "   token that was generated in the previous step. The actual token generated\n",
    "   will inform the subsequent tokens.\n",
    "\n",
    "We will repeat both functions until a special \"END OF SEQUENCE\" token is\n",
    "generated.\n",
    "\n",
    "Note that there are several tricky things that we will have to figure out.\n",
    "For example, how do we actually sample the actual token from the probability\n",
    "distribution over tokens? What would we do during training, and how might \n",
    "that be different from during testing/evaluation? We will answer those\n",
    "questions as we implement the RNN.\n",
    "\n",
    "For now, let's start with our training data.\n",
    "\n",
    "## Data: Donald Trump's Tweets from 2018\n",
    "\n",
    "The training set we use is a collection of Donald Trump's tweets from 2018.\n",
    "We will only use tweets that are 140 characters or shorter, and tweets\n",
    "that contains more than just a URL.\n",
    "Since tweets often contain creative spelling and numbers, and upper vs lower\n",
    "case characters are read very differently, we will use a character-level RNN.\n",
    "\n",
    "To start, let us load the trump.csv file to Google Colab and provide access to the drive. The file can be obtained from Quercus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "UUKaz67w3hCN",
    "outputId": "5ea3d46b-a6ac-47e9-859d-39a22f1ecd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "woTmhzIzR1VD",
    "outputId": "ff2aac99-f2d4-42c7-e8fd-26df2ced5c94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22402"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# file location (make sure to use your file location)\n",
    "file_dir = '/content/drive/My Drive/Colab Notebooks/Lab 6 Tutorial/'\n",
    "\n",
    "tweets = list(line[0] for line in csv.reader(open(file_dir + 'trump.csv')))\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGEHJRNcR1VG"
   },
   "source": [
    "There are over 20000 tweets in this collection.\n",
    "Let's look at a few of them, just to get a sense of the kind of text\n",
    "we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "HhauODvnR1VH",
    "outputId": "157d9453-6d2c-40dd-b929-29b266cb99b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God Bless the people of Venezuela!\n",
      "It was my honor. THANK YOU! https://t.co/1LvqbRQ1bi\n",
      "Nobody but Donald Trump will save Israel. You are wasting your time with these politicians and political clowns. Best! #SheldonAdelson\n"
     ]
    }
   ],
   "source": [
    "print(tweets[100])\n",
    "print(tweets[1000])\n",
    "print(tweets[10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qksavJr_R1VK"
   },
   "source": [
    "## Generating One Tweet\n",
    "\n",
    "Normally, when we build a new machine learn model, we want to make sure\n",
    "that our model can overfit. To that end, we will first build a neural network\n",
    "that can generate _one_ tweet really well. We can choose any tweet (or any other text)\n",
    "we want.  Let's choose to build an RNN that generates `tweet[100]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "u-RT-m4-R1VL",
    "outputId": "1f92ae6a-ace5-4c5c-b423-ed6742c411b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God Bless the people of Venezuela!\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "tweet = tweets[100]\n",
    "print(tweet)\n",
    "print(len(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsdHRV1uR1VP"
   },
   "source": [
    "First, we will need to encode this tweet using a one-hot encoding.\n",
    "We'll build dictionary mappings\n",
    "from the character to the index of that character (a unique integer identifier),\n",
    "and from the index to the character. We'll use the same naming scheme that `torchtext`\n",
    "uses (`stoi` and `itos`).\n",
    "\n",
    "For simplicity, we'll work with a limited vocabulary containing\n",
    "just the characters in `tweet[100]`, plus two special tokens:\n",
    "\n",
    "- `<EOS>` represents \"End of String\", which we'll append to the end of our tweet.\n",
    "  Since tweets are variable-length, this is a way for the RNN to signal\n",
    "  that the entire sequence has been generated.\n",
    "- `<BOS>` represents \"Beginning of String\", which we'll prepend to the beginning of \n",
    "  our tweet. This is the first token that we will feed into the RNN.\n",
    "\n",
    "The way we use these special tokens will become more clear as we build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InvJaRXsR1VP"
   },
   "outputs": [],
   "source": [
    "vocab = list(set(tweet)) + [\"<BOS>\", \"<EOS>\"]\n",
    "vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
    "vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "r4i3C1qs5Rrt",
    "outputId": "54e5bbad-9976-4f51-a465-baaa047b9328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'B', 't', 's', 'h', 'e', 'V', 'l', 'o', 'n', 'z', 'p', 'f', '!', 'G', ' ', 'a', 'd', '<BOS>', '<EOS>']\n",
      "{'u': 0, 'B': 1, 't': 2, 's': 3, 'h': 4, 'e': 5, 'V': 6, 'l': 7, 'o': 8, 'n': 9, 'z': 10, 'p': 11, 'f': 12, '!': 13, 'G': 14, ' ': 15, 'a': 16, 'd': 17, '<BOS>': 18, '<EOS>': 19}\n",
      "{0: 'u', 1: 'B', 2: 't', 3: 's', 4: 'h', 5: 'e', 6: 'V', 7: 'l', 8: 'o', 9: 'n', 10: 'z', 11: 'p', 12: 'f', 13: '!', 14: 'G', 15: ' ', 16: 'a', 17: 'd', 18: '<BOS>', 19: '<EOS>'}\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(vocab_stoi)\n",
    "print(vocab_itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXknqF8gR1VS"
   },
   "source": [
    "Now that we have our vocabulary, we can build the PyTorch model\n",
    "for this problem.\n",
    "The actual model is not as complex as you might think. We actually\n",
    "already learned about all the components that we need. (Using and training\n",
    "the model is the hard part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZiQsDFjR1U_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCl7ORcrR1VT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        # identiy matrix for generating one-hot vectors\n",
    "        self.ident = torch.eye(vocab_size)\n",
    "\n",
    "        # recurrent neural network\n",
    "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "        # a fully-connect layer that outputs a distribution over\n",
    "        # the next token, given the RNN output\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        inp = self.ident[inp]                  # generate one-hot vectors of input\n",
    "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
    "        output = self.decoder(output)          # predict distribution over next tokens\n",
    "        return output, hidden\n",
    "\n",
    "model = TextGenerator(vocab_size, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87pVghfCR1VV"
   },
   "source": [
    "## Training with Teacher Forcing\n",
    "\n",
    "At a very high level, we want our RNN model to have a high probability\n",
    "of generating the tweet. An RNN model generates text\n",
    "one character at a time based on the hidden state value.\n",
    "At each time step, we will check whether the mdoel generated the\n",
    "correct character. That is, at each time step,\n",
    "we are trying to select the correct next character out of all the \n",
    "characters in our vocabulary. Recall that this problem is a multi-class\n",
    "classification problem, and we can use Cross-Entropy loss to train our\n",
    "network to become better at this type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OI8aoJRpR1VX"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvgI1WQ4R1VZ"
   },
   "source": [
    "However, we don't just have a single multi-class classification problem.\n",
    "Instead, we have **one classification problem per time-step** (per token)!\n",
    "So, how do we predict the first token in the sequence? \n",
    "How do we predict the second token in the sequence? \n",
    "\n",
    "To help you understand what happens durign RNN training, we'll start with a\n",
    "inefficient training code that shows you what happens step-by-step. We'll\n",
    "start with computing the loss for the first token generated, then the second token,\n",
    "and so on.\n",
    "Later on, we'll switch to a simpler and more performant version of the code.\n",
    "\n",
    "So, let's start with the first classification problem: the problem of generating\n",
    "the **first** token (`tweet[0]`).\n",
    "\n",
    "To generate the first token, we'll feed the RNN network (with an initial, empty\n",
    "hidden state) the \"<BOS>\" token. Then, the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "p6dTQqfKR1Va",
    "outputId": "cce070f9-185f-4306-daf4-99c5f66b1b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) <class 'torch.Tensor'>\n",
      "torch.Size([1]) <class 'torch.Tensor'>\n",
      "torch.Size([1, 1]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.4225e-02,  1.3240e-01, -2.3740e-02,  3.0483e-02, -7.1641e-02,\n",
       "           1.0096e-01,  8.1199e-02,  1.2761e-01, -1.2205e-06,  3.1027e-03,\n",
       "           1.6290e-02, -5.5477e-02, -1.9324e-02,  9.2466e-03,  2.4454e-02,\n",
       "           1.1759e-01, -7.2113e-02, -1.5862e-02,  3.0889e-02, -9.1323e-02]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_input = torch.Tensor([vocab_stoi[\"<BOS>\"]])\n",
    "print(bos_input.shape, type(bos_input))\n",
    "bos_input = bos_input.long()\n",
    "print(bos_input.shape, type(bos_input))\n",
    "bos_input = bos_input.unsqueeze(0)\n",
    "print(bos_input.shape, type(bos_input))\n",
    "output, hidden = model(bos_input, hidden=None)\n",
    "output # distribution over the first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9pLljlLGiw52",
    "outputId": "f46064d0-b33d-4a2a-b951-3c9f17947fa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uVmBrFLVR1Vd"
   },
   "source": [
    "We can compute the loss using `criterion`. Since the model is untrained,\n",
    "the loss is expected to be high. (For now, we won't do anything\n",
    "with this loss, and omit the backward pass.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BN_FopQYR1Ve",
    "outputId": "113f51da-869c-4f17-a6dc-1b0f777088c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0706, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.Tensor([vocab_stoi[tweet[0]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "L8M72rsz66Ar",
    "outputId": "d91ee36b-ea22-4a38-9297-f9a2e0088d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6]])\n",
      "tensor([[[-0.0023,  0.1443,  0.0212,  0.0992,  0.1040,  0.0890, -0.0478,\n",
      "           0.0194,  0.1120, -0.0436,  0.0201,  0.0630, -0.0489,  0.0486,\n",
      "           0.1207, -0.0904, -0.0836,  0.0305, -0.0409, -0.0220]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[-0.0023,  0.1443,  0.0212,  0.0992,  0.1040,  0.0890, -0.0478,  0.0194,\n",
      "          0.1120, -0.0436,  0.0201,  0.0630, -0.0489,  0.0486,  0.1207, -0.0904,\n",
      "         -0.0836,  0.0305, -0.0409, -0.0220]], grad_fn=<AsStridedBackward>)\n",
      "tensor([6])\n"
     ]
    }
   ],
   "source": [
    "print(target)\n",
    "print(output)\n",
    "print(output.reshape(-1, vocab_size))\n",
    "print(target.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jr65KC4VR1Vg"
   },
   "source": [
    "Now, we need to update the hidden state and generate a prediction\n",
    "for the next token. To do so, we need to provide the current token to\n",
    "the RNN. We already said that during test time, we'll need to sample\n",
    "from the predicted probabilty over tokens that the neural network\n",
    "just generated. \n",
    "\n",
    "Right now, we can do something better: we can **use the ground-truth,\n",
    "actual target token**. This technique is called **teacher-forcing**, \n",
    "and generally speeds up training. The reason is that right now, \n",
    "since our model does not perform well, the predicted probability\n",
    "distribution is pretty far from the ground truth. So, it is very,\n",
    "very difficult for the neural network to get back on track given bad\n",
    "input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "oq5PfJvhR1Vh",
    "outputId": "8f25682a-da54-4121-be94-3c66c9002977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0403,  0.1374,  0.0388,  0.0839,  0.1198,  0.0802, -0.0541,\n",
       "           0.0267,  0.0691, -0.0618, -0.0191,  0.0998, -0.0151,  0.0469,\n",
       "           0.1330, -0.0750, -0.0906,  0.0467, -0.0325, -0.0012]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use teacher-forcing: we pass in the ground truth `target`,\n",
    "# rather than using the NN predicted distribution\n",
    "output, hidden = model(target, hidden)\n",
    "output # distribution over the second token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U13rWRdzR1Vk"
   },
   "source": [
    "Similar to the first step, we can compute the loss, quantifying the\n",
    "difference between the predicted distribution and the actual next\n",
    "token. This loss can be used to adjust the weights of the neural\n",
    "network (which we are not doing yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bcrB_W1rR1Vl",
    "outputId": "e451ee3a-c47a-4cc1-bb04-beb101bad08b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8854, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nXcTfTDR1Vn"
   },
   "source": [
    "We can continue this process of:\n",
    "\n",
    "- feeding the previous ground-truth token to the RNN,\n",
    "- obtaining the prediction distribution over the next token, and\n",
    "- computing the loss,\n",
    "\n",
    "for as many steps as there are tokens in the ground-truth tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JPIFaN99R1Vo",
    "outputId": "0ef01ece-413d-41b8-a063-ea17f3dd6fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([[[-0.0321,  0.1427,  0.0448,  0.1036,  0.1191,  0.0974, -0.0968,\n",
      "           0.0005,  0.0566, -0.0691, -0.0189,  0.1206, -0.0185, -0.0102,\n",
      "           0.1500, -0.0580, -0.0986,  0.0828, -0.0334, -0.0036]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8801, grad_fn=<NllLossBackward>)\n",
      "3 tensor([[[-0.0241,  0.1407,  0.0507,  0.1155,  0.1168,  0.1062, -0.1245,\n",
      "          -0.0156,  0.0492, -0.0770, -0.0265,  0.1350, -0.0186, -0.0439,\n",
      "           0.1625, -0.0474, -0.1015,  0.1005, -0.0278, -0.0061]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8819, grad_fn=<NllLossBackward>)\n",
      "4 tensor([[[-0.0192,  0.1372,  0.0549,  0.1213,  0.1152,  0.1095, -0.1419,\n",
      "          -0.0246,  0.0458, -0.0836, -0.0334,  0.1448, -0.0177, -0.0633,\n",
      "           0.1709, -0.0411, -0.1021,  0.1098, -0.0227, -0.0083]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8851, grad_fn=<NllLossBackward>)\n",
      "5 tensor([[[-0.0167,  0.1342,  0.0575,  0.1237,  0.1144,  0.1102, -0.1524,\n",
      "          -0.0295,  0.0445, -0.0885, -0.0380,  0.1512, -0.0166, -0.0742,\n",
      "           0.1762, -0.0375, -0.1018,  0.1149, -0.0193, -0.0099]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8880, grad_fn=<NllLossBackward>)\n",
      "6 tensor([[[-0.0154,  0.1320,  0.0590,  0.1246,  0.1140,  0.1099, -0.1585,\n",
      "          -0.0321,  0.0443, -0.0918, -0.0406,  0.1552, -0.0158, -0.0802,\n",
      "           0.1794, -0.0356, -0.1014,  0.1177, -0.0173, -0.0109]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8900, grad_fn=<NllLossBackward>)\n",
      "7 tensor([[[-0.0148,  0.1305,  0.0598,  0.1247,  0.1139,  0.1095, -0.1619,\n",
      "          -0.0336,  0.0445, -0.0940, -0.0419,  0.1575, -0.0151, -0.0834,\n",
      "           0.1811, -0.0346, -0.1009,  0.1191, -0.0163, -0.0115]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8914, grad_fn=<NllLossBackward>)\n",
      "8 tensor([[[-0.0145,  0.1296,  0.0602,  0.1247,  0.1138,  0.1091, -0.1638,\n",
      "          -0.0344,  0.0447, -0.0952, -0.0425,  0.1588, -0.0147, -0.0851,\n",
      "           0.1820, -0.0340, -0.1006,  0.1199, -0.0158, -0.0119]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8923, grad_fn=<NllLossBackward>)\n",
      "9 tensor([[[-0.0143,  0.1290,  0.0604,  0.1246,  0.1138,  0.1089, -0.1648,\n",
      "          -0.0349,  0.0450, -0.0960, -0.0426,  0.1594, -0.0144, -0.0860,\n",
      "           0.1824, -0.0338, -0.1005,  0.1202, -0.0156, -0.0120]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8928, grad_fn=<NllLossBackward>)\n",
      "10 tensor([[[-0.0142,  0.1287,  0.0605,  0.1245,  0.1137,  0.1089, -0.1653,\n",
      "          -0.0352,  0.0452, -0.0964, -0.0426,  0.1598, -0.0142, -0.0864,\n",
      "           0.1826, -0.0337, -0.1004,  0.1203, -0.0155, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8931, grad_fn=<NllLossBackward>)\n",
      "11 tensor([[[-0.0141,  0.1285,  0.0605,  0.1244,  0.1137,  0.1088, -0.1656,\n",
      "          -0.0353,  0.0453, -0.0966, -0.0425,  0.1599, -0.0141, -0.0866,\n",
      "           0.1827, -0.0337, -0.1003,  0.1204, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8933, grad_fn=<NllLossBackward>)\n",
      "12 tensor([[[-0.0141,  0.1283,  0.0605,  0.1244,  0.1136,  0.1088, -0.1657,\n",
      "          -0.0354,  0.0454, -0.0968, -0.0425,  0.1600, -0.0140, -0.0867,\n",
      "           0.1827, -0.0337, -0.1003,  0.1203, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8934, grad_fn=<NllLossBackward>)\n",
      "13 tensor([[[-0.0140,  0.1283,  0.0605,  0.1244,  0.1136,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0455, -0.0968, -0.0424,  0.1600, -0.0140, -0.0868,\n",
      "           0.1827, -0.0337, -0.1003,  0.1203, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8935, grad_fn=<NllLossBackward>)\n",
      "14 tensor([[[-0.0140,  0.1282,  0.0605,  0.1244,  0.1136,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0456, -0.0969, -0.0424,  0.1600, -0.0140, -0.0868,\n",
      "           0.1826, -0.0337, -0.1003,  0.1203, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8935, grad_fn=<NllLossBackward>)\n",
      "15 tensor([[[-0.0140,  0.1282,  0.0605,  0.1244,  0.1135,  0.1089, -0.1658,\n",
      "          -0.0355,  0.0456, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1202, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8935, grad_fn=<NllLossBackward>)\n",
      "16 tensor([[[-0.0140,  0.1282,  0.0605,  0.1244,  0.1135,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0456, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1202, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "17 tensor([[[-0.0140,  0.1282,  0.0605,  0.1244,  0.1135,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0456, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1202, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "18 tensor([[[-0.0140,  0.1282,  0.0605,  0.1244,  0.1135,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1202, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "19 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1202, -0.0154, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "20 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1089, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1202, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "21 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1826, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "22 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "23 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "24 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "25 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "26 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "27 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "28 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "29 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "30 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "31 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "32 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "33 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.8936, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, len(tweet)):\n",
    "    output, hidden = model(target, hidden)\n",
    "    target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
    "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                     target.reshape(-1))             # reshape to 1D tensor\n",
    "    print(i, output, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPcGN8T9R1Vr"
   },
   "source": [
    "Finally, with our final token, we should expect to output the \"<EOS>\"\n",
    "token, so that our RNN learns when to stop generating characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "mNpcFt7MR1Vs",
    "outputId": "ffe29f05-aaca-4e7c-a88b-982859ff2dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 tensor([[[-0.0140,  0.1282,  0.0606,  0.1244,  0.1135,  0.1090, -0.1657,\n",
      "          -0.0355,  0.0457, -0.0969, -0.0423,  0.1600, -0.0139, -0.0868,\n",
      "           0.1825, -0.0337, -0.1004,  0.1201, -0.0153, -0.0121]]],\n",
      "       grad_fn=<AddBackward0>) tensor(3.0338, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = model(target, hidden)\n",
    "target = torch.Tensor([vocab_stoi[\"<EOS>\"]]).long().unsqueeze(0)\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor\n",
    "print(i, output, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9izimdzFR1Vv"
   },
   "source": [
    "In practice, we don't really need a loop. Recall that in a predictive RNN,\n",
    "the `nn.RNN` module can take an entire sequence as input. We can do the\n",
    "same thing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Sd9MhPC-R1Vw",
    "outputId": "fe356cb1-fb08-4efb-ae89-765216b4b558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36])\n"
     ]
    }
   ],
   "source": [
    "tweet_ch = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
    "tweet_indices = [vocab_stoi[ch] for ch in tweet_ch]\n",
    "tweet_tensor = torch.Tensor(tweet_indices).long().unsqueeze(0)\n",
    "\n",
    "print(tweet_tensor.shape)\n",
    "\n",
    "output, hidden = model(tweet_tensor[:,:-1]) # <EOS> is never an input token\n",
    "target = tweet_tensor[:,1:]                 # <BOS> is never a target token\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWOEipAcR1Vz"
   },
   "source": [
    "Here, the input to our neural network model is the *entire*\n",
    "sequence of input tokens (everything from \"<BOS>\" to the\n",
    "last character of the tweet). The neural network generates a prediction distribution\n",
    "of the next token at each step. We can compare each of these  with the ground-truth\n",
    "`target`.\n",
    "\n",
    "\n",
    "Our training loop (for learning to generate the single `tweet`) will therefore\n",
    "look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "MnDk217g9Paf",
    "outputId": "a7ac524b-a432-41cf-b5f0-fe5c2146cdc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18,  6,  1,  8, 15,  9, 17,  7,  3,  3, 15, 10,  2,  7, 15,  0,  7,  1,\n",
      "          0, 17,  7, 15,  1,  4, 15,  5,  7, 16,  7, 11, 12,  7, 17, 13, 14]])\n",
      "tensor([[ 6,  1,  8, 15,  9, 17,  7,  3,  3, 15, 10,  2,  7, 15,  0,  7,  1,  0,\n",
      "         17,  7, 15,  1,  4, 15,  5,  7, 16,  7, 11, 12,  7, 17, 13, 14, 19]])\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tensor[:,:-1])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "7HG6kNxDR1Vz",
    "outputId": "1938effe-472b-48ff-d25f-7d5be6e0f130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 100] Loss 1.840927\n",
      "[Iter 200] Loss 0.157599\n",
      "[Iter 300] Loss 0.030487\n",
      "[Iter 400] Loss 0.013288\n",
      "[Iter 500] Loss 0.007862\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for it in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(tweet_tensor[:,:-1])\n",
    "    loss = criterion(output.reshape(-1, vocab_size),\n",
    "                 target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (it+1) % 100 == 0:\n",
    "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lADJ6zO0R1V2"
   },
   "source": [
    "The training loss is decreasing with training, which is what we expect.\n",
    "\n",
    "## Generating a Token\n",
    "\n",
    "At this point, we want to see whether our model is actually learning\n",
    "something. So, we need to talk about how to\n",
    "actually use the RNN model to generate text. If we can \n",
    "generate text, we can make a qualitative asssessment of how well\n",
    "our RNN is performing.\n",
    "\n",
    "The main difference between training and test-time (generation time)\n",
    "is that we don't have the ground-truth tokens to feed as inputs\n",
    "to the RNN. Instead, we need to actually **sample** a token based\n",
    "on the neural network's prediction distribution.\n",
    "\n",
    "But how can we sample a token from a distribution?\n",
    "\n",
    "On one extreme, we can always take\n",
    "the token with the largest probability (argmax). This has been our\n",
    "go-to technique in other classification tasks. However, this idea\n",
    "will fail here. The reason is that in practice, \n",
    "**we want to be able to generate a variety of different sequences from\n",
    "the same model**. An RNN that can only generate a single new Trump Tweet\n",
    "is fairly useless.\n",
    "\n",
    "In short, we want some randomness. We can do so by using the logit\n",
    "outputs from our model to construct a multinomial distribution over\n",
    "the tokens, then and sample a random token from that multinomial distribution.\n",
    "\n",
    "One natural multinomial distribution we can choose is the \n",
    "distribution we get after applying the softmax on the outputs.\n",
    "However, we will do one more thing: we will add a **temperature**\n",
    "parameter to manipulate the softmax outputs. We can set a\n",
    "**higher temperature** to make the probability of each token\n",
    "**more even** (more random), or a **lower temperature** to assign\n",
    "more probability to the tokens with a higher logit (output).\n",
    "A **higher temperature** means that we will get a more diverse sample,\n",
    "with potentially more mistakes. A **lower temperature** means that we\n",
    "may see repetitions of the same high probability sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "YCBOf-UlR1V3",
    "outputId": "7e39619c-2965-4fc8-ba19-2314c465b614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God Bless the people of Venezuela!\n",
      "God Bless the people of Venezuela!\n",
      "os Bless the peopleeof Venezuela!\n",
      "God Bdess Bpeooeop e ofzuenezeela!\n",
      "auodlflBdptfh oe!on!!lhlpfeVeGhfpup!f\n"
     ]
    }
   ],
   "source": [
    "def sample_sequence(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\"\n",
    "   \n",
    "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "        \n",
    "        if predicted_char == \"<EOS>\":\n",
    "            break\n",
    "        generated_sequence += predicted_char       \n",
    "        inp = torch.Tensor([top_i]).long()\n",
    "    return generated_sequence\n",
    "\n",
    "print(sample_sequence(model, temperature=0.8))\n",
    "print(sample_sequence(model, temperature=1.0))\n",
    "print(sample_sequence(model, temperature=1.5))\n",
    "print(sample_sequence(model, temperature=2.0))\n",
    "print(sample_sequence(model, temperature=5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61I-4wHHR1WB"
   },
   "source": [
    "Since we only trained the model on a single sequence, we won't see\n",
    "the effect of the temperature parameter yet. \n",
    "\n",
    "For now, the output of the calls to the `sample_sequence` function\n",
    "assures us that our training code looks reasonable, and we can\n",
    "proceed to training on our full dataset!\n",
    "\n",
    "## Training the Trump Tweet Generator\n",
    "\n",
    "For the actual training, let's use `torchtext` so that we can use\n",
    "the `BucketIterator` to make batches. Like in Lab 5, we'll create a \n",
    "`torchtext.data.Field` to use `torchtext` to read the CSV file, and convert\n",
    "characters into indices. The object has convient parameters to specify\n",
    "the BOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5TaOu21fR1WC",
    "outputId": "bc7dc09e-3dcd-4211-8660-a00a3ff680d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22402"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
    "                                  include_lengths=True, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True,       # to turn each character into an integer index\n",
    "                                  init_token=\"<BOS>\",   # BOS token\n",
    "                                  eos_token=\"<EOS>\")    # EOS token\n",
    "\n",
    "fields = [('text', text_field), ('created_at', None), ('id_str', None)]\n",
    "trump_tweets = torchtext.data.TabularDataset(file_dir + \"trump.csv\", \"csv\", fields)\n",
    "len(trump_tweets) # should be >20,000 like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4Q-fvtXAR1WE",
    "outputId": "cc5a9f9e-c08b-42fc-e284-e5aa317c68ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(trump_tweets)\n",
    "vocab_stoi = text_field.vocab.stoi # so we don't have to rewrite sample_sequence\n",
    "vocab_itos = text_field.vocab.itos # so we don't have to rewrite sample_sequence\n",
    "vocab_size = len(text_field.vocab.itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fVuKmNIR1WH"
   },
   "source": [
    "Let's just verify that the `BucketIterator` works as expected, but start with batch_size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "_UL64eVUR1WI",
    "outputId": "ef81dfab-c774-4893-85db-9bb47c98cab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([82])\n",
      "torch.Size([1, 82])\n"
     ]
    }
   ],
   "source": [
    "data_iter = torchtext.data.BucketIterator(trump_tweets, \n",
    "                                          batch_size=1,\n",
    "                                          sort_key=lambda x: len(x.text),\n",
    "                                          sort_within_batch=True)\n",
    "for (tweet, lengths), label in data_iter:\n",
    "    print(label)   # should be None\n",
    "    print(lengths) # contains the length of the tweet(s) in batch\n",
    "    print(tweet.shape) # should be [1, max(length)]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pftGNL97R1WK"
   },
   "source": [
    "To account for batching, our actual training code will change, but just a little bit.\n",
    "In fact, our training code from before will work with a batch size larger than one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLx8pz5mR1WL"
   },
   "outputs": [],
   "source": [
    "def train(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    it = 0\n",
    "    \n",
    "    data_iter = torchtext.data.BucketIterator(data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              sort_key=lambda x: len(x.text),\n",
    "                                              sort_within_batch=True)\n",
    "    for e in range(num_epochs):\n",
    "        # get training set\n",
    "        avg_loss = 0\n",
    "        for (tweet, lengths), label in data_iter:\n",
    "            target = tweet[:, 1:]\n",
    "            inp = tweet[:, :-1]\n",
    "            # cleanup\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output, _ = model(inp)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            it += 1 # increment iteration count\n",
    "            if it % print_every == 0:\n",
    "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
    "                print(\"    \" + sample_sequence(model, 140, 0.8))\n",
    "                avg_loss = 0\n",
    "\n",
    "model = TextGenerator(vocab_size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JUNZfzbb_0_s",
    "outputId": "8955a7b9-beb0-4ef1-a58b-73e7c7cce0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 101] Loss 3.767316\n",
      "    2😇ele!Bt d  rtduxhn car:n ndinnt ahertann  ociiwadearicaaNnl.nawd dyogtenrtmeytrr iErire tiaennRse n ean0nara0u    a  d tinmre reri  oed nhd\n",
      "[Iter 201] Loss 3.389243\n",
      "    Pesan dhhamon 1l/euito o s w Uve r of. hunarthisn giNilZvem p hpl k ahurerumsane ci!d Mrog#: \n",
      "[Iter 301] Loss 3.136456\n",
      "    @ufe!2 mos @r four mp:s @fre neey cE ory jin llll ItTmp fDy1tp fe ICearlrutiany 1tpsulanin TRlgd Kurrimp @Dorampr . t ppasE1/t thonalouf 1ro\n",
      "[Iter 401] Loss 2.921844\n",
      "    ho den /f jome hit! its ig . 1sentor Iruth intimp As tho/Fk0coum in wo//t.Fouscowtco////ttptprupave OForit. t..\n",
      "[Iter 501] Loss 2.827049\n",
      "    fu! jores wit ho/m://t.chaGmorgpimpe Creds mutho seay @S Dui I Trand At deall het y inThigh yewl bisttyegtot yrusterunadeatop ct enallicles \n",
      "[Iter 601] Loss 2.690218\n",
      "    'te woure de perttisellen nored indere fburs Greni are inder ind ind henar on I Arrarser//t.co/W..\n",
      "[Iter 701] Loss 2.547648\n",
      "    ruliscees ine so dabanditeristand agt seid fof the thestou thang sors on to foor and bibe to the the theritees thon she mi g wore dempco tou\n",
      "[Iter 801] Loss 2.562896\n",
      "    I Oanal in SA Of and thall the in thee sa toeed in mencar 🐘ay Ang thon He mump ht bemening tlectth the Hald gor tin geor Ts!\n",
      "[Iter 901] Loss 2.495502\n",
      "    Anm shigeing CRionaldTwimh are ssen sook8 wy cay hare the cresirefill Pull bathe wist Sigiss\n",
      "[Iter 1001] Loss 2.428760\n",
      "    @alakiss al  at for dant thic fun cor hand mt tho https://t..co/mWD5x\n",
      "[Iter 1101] Loss 2.370621\n",
      "    @realDon.… Be bl goien then as our most oproy in noss lest nove ass to weo wand to htanss wide the here soust dortistow be go preston.\n",
      "[Iter 1201] Loss 2.377675\n",
      "    –r a to and he Amica norrus. @realDonaldTrump on of that ine.\n",
      "[Iter 1301] Loss 2.314831\n",
      "    Thing ding erintand #Trulp a? the bod thting loon I hoty of leow bile bere a merick I sering!\n",
      "[Iter 1401] Loss 2.297814\n",
      "    Thing the the to Cherreciller amp Korders thes Hamp; all bate in a Gray Wous Chain whis... vicate mang the theat mast ho micers andin I Nexp\n",
      "[Iter 1501] Loss 2.311950\n",
      "    @nto Nonds will be will http://t.Wo/JLjglORT\n",
      "[Iter 1601] Loss 2.232879\n",
      "    .@VeAenerands @DonaldTrump of ald't horely the Suberet medade wart wi tould megats the s aked wew GEL WAKI. GISS CA.\n",
      "[Iter 1701] Loss 2.214655\n",
      "    With Stery ave millice to ald the besicnerts ofron't b an of antace great bat- ht. #MeH40mSa\n",
      "[Iter 1801] Loss 2.184033\n",
      "    @Goackenttivegary: “Free heseders sous to of Shou arbating for been Clend\n",
      "[Iter 1901] Loss 2.175687\n",
      "    @BestySinn: @realDonaldTrump -  as chaspery be wall prepper Leantarid hise thal Cane be treis ad the you Aprey! https://t.co/4M2V5ziznissey \n",
      "[Iter 2001] Loss 2.200416\n",
      "    @ol: @realDonaldTrump or sting un! I that wot the aF far And you shis the ie he #Trumpprever the gomend https://t.co/AHWzYHlvGU\n",
      "[Iter 2101] Loss 2.186832\n",
      "    @Meryatcou: @realDonaldTrump @realDonaldTrump Great.\n",
      "[Iter 2201] Loss 2.116110\n",
      "    Thank you Justion me are and op tooke to @realDonardTrump un are con arachas baut!\n",
      "[Iter 2301] Loss 2.136087\n",
      "    Canz3 - ALGTHAR EALL\n",
      "[Iter 2401] Loss 2.112298\n",
      "    Truaking to rews Cougricagaina. Peraling be want weal Pense for arond ove portinn. Sayovere wank. Serope. of the be the wash gemmst a grey t\n",
      "[Iter 2501] Loss 2.113081\n",
      "    Withtines but you On AThingery Congrifnice and bem the all Coren Will is affors to Co but the Rime I worker poongovald the presess NOT A Tru\n",
      "[Iter 2601] Loss 2.109412\n",
      "    Great of Will of to gar meattime fiems to key I wiss lither Sis disiendes  https://t.co/gdhPfyn8Rmt\n",
      "[Iter 2701] Loss 2.102566\n",
      "    @Wcerbelaoncanrento the @leengroncing @Fondutchackintheswe @realDonaldTrump @asedTrunk Thcan mia mm in prongres\n",
      "[Iter 2801] Loss 2.149826\n",
      "    Tha well wing @ManYoreation: @realDonaldTrump would goul with the Hice bac the ray!\n",
      "[Iter 2901] Loss 2.096714\n",
      "    Great Seafion wht dite on op are frelled Amery Bell#Minglima Preston Une it that Sprookion https://t.co/roajwZS\n",
      "[Iter 3001] Loss 2.060033\n",
      "    Markanishing in Miden al me on @ReSTRNLPIN!\n",
      "[Iter 3101] Loss 2.054792\n",
      "    Theake a whis way the vest and evere for of I NET HON The bada ard toped sece shot lave refore @carkicowe just Sanis!\n",
      "[Iter 3201] Loss 1.981961\n",
      "    Just inter our the COnJass wing so doning as Dewars cevent to succen sed of the @realDonaldTrump the incenteruse comcorrig foratare latides \n",
      "[Iter 3301] Loss 2.074540\n",
      "    @bylackion_: @realDonaldTrump for @keelanapris Vicher you hoow crowd a chiebrotice commine of likeess you in a seakewant a fropres crent! #T\n",
      "[Iter 3401] Loss 2.041041\n",
      "    @Rveerinkedery: @reridyGrealDonaldTrump pook yeely gete) of will the its Wast in like wonk you!\n",
      "[Iter 3501] Loss 2.049273\n",
      "    @AlElJian Ronvs: be the cal areary nat in Decent can the wiffing not are plis he reast a Obammazy with supsen on the best to Trump httpa: ht\n",
      "[Iter 3601] Loss 2.058419\n",
      "    I were dean nece\" thiteres years overy in Clanmand you is to vegatio and. Look the now! It's Dasure and tonida I want of this a no the @EBCl\n",
      "[Iter 3701] Loss 2.017290\n",
      "    @Tre @rea: @realDonaldTrump Besige! https://t.co/ckweeeiSC\n",
      "[Iter 3801] Loss 2.020236\n",
      "    \"@Trumpinting thet jeled es werurd and ack at $18 to meDont condical and a swal spow laedice about man say will our thever Parene.\n",
      "[Iter 3901] Loss 1.986475\n",
      "    Just her bell bement you Flang whe love!\n",
      "[Iter 4001] Loss 1.961353\n",
      "    America I tell stating dobie op. The dect brentemed bele shower palesineelding all all be sery all hes I are to all torader up with to vetee\n",
      "[Iter 4101] Loss 1.944171\n",
      "    @cisean11: @realDonaldTrump the siand gow the great speem un for ffrce boonss fir ban congrating  man and fill and  #MakeAmenicastan Trump  \n",
      "[Iter 4201] Loss 2.035008\n",
      "    @nigappike Gor: “I we ime agoing a elace wesing in @realDonaldTrump you bribe at exomione in gois mouth all be WYarting\n",
      "[Iter 4301] Loss 2.029141\n",
      "    The keat has expeewed is of #Trump2016 is one for Repunterce of in Conor- sumprost to Cannold be them.\n",
      "[Iter 4401] Loss 2.009469\n",
      "    @klingan@ling: @realDonaldTrump de ader forature lover promble pressiend http://t.co/8LxgYCx1\n",
      "[Iter 4501] Loss 2.041313\n",
      "    Presiden @Lothissuldine https://t.co/_sW3iyxvurghut has in a clang what speed\n",
      "[Iter 4601] Loss 2.006514\n",
      "    A beally Medingenterr ases a the sooking on intely and for my the tunter recold fact putchents! https://t.co/4LuvBAbajD\n",
      "[Iter 4701] Loss 2.020717\n",
      "    201000000000000000000000 menillions despaking wasch its the in ISwal to decay a time Inde to the the rort.\n",
      "[Iter 4801] Loss 1.943172\n",
      "    Thas in sadk theaking hespofs  New Jebs Inonald Clinton a be is you. Toust him on is illy of to Cornin watconst from and me on @LiCCLOWE VIC\n",
      "[Iter 4901] Loss 2.045920\n",
      "    @dougully DonaldTrump https://t.co/KWhOzk8l0\n",
      "[Iter 5001] Loss 2.042001\n",
      "    @Jposol7000: @realDonaldTrump @realDonaldTrump diue!\n",
      "[Iter 5101] Loss 1.966744\n",
      "    Trump interes Has he! #Trump2016 http://t.co/H3WiJeREmK\n",
      "[Iter 5201] Loss 1.965084\n",
      "    @NAGLewMDonald @realDonaldTrump West ball deazing bead EBust with thein the Make https://t.co/xpi7ddg71\n",
      "[Iter 5301] Loss 1.979766\n",
      "    Thanks: @realDonaldTrump @fansi1 dy my speag.  I wize wonging not me. As chomportian this at the would be poll are wanting and ruckitce!\n",
      "[Iter 5401] Loss 1.943457\n",
      "    Mely U...Mars Bet and start andor is a could! #AmericaFricahing be are morl who dess in @Fiverffice The Kanes.\n",
      "[Iter 5501] Loss 1.917548\n",
      "    @kinnerew: @realDonaldTrump would denthing bin will be in I they this on theare as can for monoure titald you true thank you for loint Senti\n",
      "[Iter 5601] Loss 1.973956\n",
      "    It was me has Cunterre Corking So cord rigat in this a the to support thowe o repart theak on ark I want onfire enade...!\n",
      "[Iter 5701] Loss 1.878816\n",
      "    Vistorning out becaust inney for ade on @Mintintlinton @ChHizericaC out for president fir! @FoxNNN\n",
      "[Iter 5801] Loss 1.970407\n",
      "    Thank you @realDonaldTrump Hauds!\n",
      "[Iter 5901] Loss 1.969431\n",
      "    I spoce the a ng acking are interver the Carward interons were the and the stAninging with thened and your realing people. the have great an\n",
      "[Iter 6001] Loss 1.938377\n",
      "    I want #Trump2016 (to the DOU. #TrumpPotghtet at With Ameridare for my wore we sall sivery bectesh comsting should all are eregrow!\n",
      "[Iter 6101] Loss 1.934615\n",
      "    Thank you. . Yout agrat think net a neahing the  Thanks.\n",
      "[Iter 6201] Loss 1.969402\n",
      "    The @Gimenast TA NEW HANK YOU OUST so tha @realDonaldTrump het we tooking. The Borking bed it of me with http://t.co/4pwVsecp2\n",
      "[Iter 6301] Loss 1.931038\n",
      "    The for replice sut of my in MILL GREAT AGAIN!#ThanLPargimp. Stated to thanduy to lay but Trump in soll was all betriter to so be thet Gols.\n",
      "[Iter 6401] Loss 1.914809\n",
      "    @sobdrollity: @senaliliasiole @woppel Jears and for the president estopler the treate on @canHe N!\n",
      "[Iter 6501] Loss 1.937318\n",
      "    A.OK Amaye worser and of thime vooting to for to negies world for Thank you!\n",
      "[Iter 6601] Loss 1.950537\n",
      "    Enjoy nabsidan trourd to reading I day. I want no look st aronet a greal reced the step Stated?\n",
      "[Iter 6701] Loss 1.912881\n",
      "    @Setthamp: @realDonaldTrump Taye Normenting what this stopp age @BruofyHas my day. Hith sun the sway! You @realDonaldTrump hately don's to b\n",
      "[Iter 6801] Loss 1.938662\n",
      "    @telkeGranyso8 @realDonaldTrump Will's bad fos on lod and be thanks.\n",
      "[Iter 6901] Loss 1.954996\n",
      "    @erintsug: Ind us to the @Apprented Tun @Aberrttopink you. Wance Sechiston with a the on gover say! #DeaGAGA🇺🇸\n",
      "[Iter 7001] Loss 1.925365\n",
      "    Thinge Rosdatie? @Gouznitican hepped you a been. Sonted at belated for shat everyming the deal couldrest of that on the would love to prosen\n",
      "[Iter 7101] Loss 1.868972\n",
      "    The @NN said! https://t.co/dVIOeQIYd\n",
      "[Iter 7201] Loss 1.996922\n",
      "    Just cears in groung herect of you. We an hericated need illy hare the beeing chign. Thanks atiows! Thank you.\n",
      "[Iter 7301] Loss 1.907601\n",
      "    .....M.\n",
      "[Iter 7401] Loss 1.889260\n",
      "    @Iinmetin: @realDonaldTrump I fact to just of I well in gon the Trump in secain recording or of that in of is &amp; Interchize!!! https://t.\n",
      "[Iter 7501] Loss 1.897319\n",
      "    @damaddz: @reaDDonaldTrump!\n",
      "[Iter 7601] Loss 1.933749\n",
      "    Via @TrumpJorden Prieaws SREpT AMER EAIE SAN 25 PHELT ALA https://t.co/VRvdkHeEw\n",
      "[Iter 7701] Loss 1.914040\n",
      "    Thank you yeet the run the have and with the illy mo read on ame! Sty!\n",
      "[Iter 7801] Loss 1.922232\n",
      "    @jaicutimel: @realDonaldTrump will be and to is all\n",
      "[Iter 7901] Loss 1.960232\n",
      "    Thanks Mo Trump and itworcousteskning cam Obusting News. Ameract laskning do never Fox! It Reputing that 7. Abark the at Reso the knew rusin\n",
      "[Iter 8001] Loss 1.908872\n",
      "    Rase't more the goen the hercoller a great my \"is your that crung to country many! https://t.co/pEc9ZRgNt\n",
      "[Iter 8101] Loss 1.914550\n",
      "    : @realDonaldTrump Is state - at poon has beter alsory of his of to like - stress to Bust him deem.\n",
      "[Iter 8201] Loss 1.902908\n",
      "    Frioplant on record to they was trump to \"bade the Hillary be like the mated offer tate satoll in I and conner it! There in people to manney\n",
      "[Iter 8301] Loss 1.867394\n",
      "    “Donald Trump’s big into - be the great this torking @Leaplenn” http://t.peoleahilts. The bected wall : http://t.co/HinTMLn0Lw\n",
      "[Iter 8401] Loss 1.918368\n",
      "    @Shoon2525: Mr president it bive are will Getarrion @realDonaldTrump Goving neaden the is so Veter Very\n",
      "[Iter 8501] Loss 1.862202\n",
      "    . - @ASprented HimPe will Pariever a mastromy kiads to Pruenney  https://t.co/5cKo58x7FP\n",
      "[Iter 8601] Loss 1.904999\n",
      "    @traalsesenp: @Me_vabys @ore_for our of New https://t.co/ZxTv0qnXk\n",
      "[Iter 8701] Loss 1.857661\n",
      "    @Fillpp1ongelvinis @sentank @TrumpPhinaFrooked 2016 by Warrsor. TOApL York Really move thook to erentiont will be about you time to says lew\n",
      "[Iter 8801] Loss 1.914222\n",
      "    Apleaves candican the Pleasing suceling for cid give thing not great winht shopen tho like tone by with the have for president fifestint!\n",
      "[Iter 8901] Loss 1.818594\n",
      "    Shas releet to wen the #Trump2016https://t.co/ABIUtJvT\n",
      "[Iter 9001] Loss 1.851441\n",
      "    @Foxnpentans @CarresTile Magates  a go will by Plearinots of why ent on the Trump boing topeing say the did neve the me deints our great.\n",
      "[Iter 9101] Loss 1.828960\n",
      "    @Dibsollillex: Reconor a great the Witch they we on cording. #taked. @realDonaldTrump he vote speaks be be the kied o @Nep24\n",
      "[Iter 9201] Loss 1.861739\n",
      "    Yous with thank you to me intervib the Fongrantin'CAmerica. Cong @Gohk_pieppreshbaire thing 20 chue losing bacce puonf I've!\n",
      "[Iter 9301] Loss 1.852119\n",
      "    @Driagherer7: @realDonaldTrump Trump Dack course! #Iverand #AjstaryKar #SATV hepresting a by U.S.\n",
      "[Iter 9401] Loss 1.849245\n",
      "    Agall of your nered a forinury for Preed Nomth awory has! https://t.co/WoPU0OZ\n",
      "[Iter 9501] Loss 1.890675\n",
      "    What you lore out 1:00 FAM ASEMS JOT #Trump2016\n",
      "[Iter 9601] Loss 1.925244\n",
      "    Waskel untrum it crow it you a great ittast the Day is ingeto the say you the wate (president I like lo be great you!\n",
      "[Iter 9701] Loss 1.855518\n",
      "    I on @Mamionns whew entically a forathing who is beitevers with to concers tonging dikid!\n",
      "[Iter 9801] Loss 1.913804\n",
      "    Obama scompilled not the and seed to is by Son has and on to be moring a for Fix &amp; @realDonaldTrump presidention law tombring. The place\n",
      "[Iter 9901] Loss 1.922331\n",
      "    @Veneimbernie: @realDonaldTrump you in Chican. Boded vote! #Trump2016 my promutions. Time (and the from Callly http://t.co/1574owurDK\n",
      "[Iter 10001] Loss 1.895693\n",
      "    Look on and not in Presidentie.P.L.K. Light who going is President. What is nothing there my discord Conald Trump will of enctidesive!\n",
      "[Iter 10101] Loss 1.898832\n",
      "    Big the Hillary BU Live’s be tonork only chite abatemmy stuce 1) to to 3 have finels in the WANS EFurly. We thank of count to about wore pro\n",
      "[Iter 10201] Loss 1.852812\n",
      "    American only standing the beed not is and ADerobional #AmericaGreatAgain https://t.co/2UPMEEwHIW\n",
      "[Iter 10301] Loss 1.877014\n",
      "    @USterarnefortiod: @realDonaldTrump Make AMerica! #Trump16 I will be lest the plisi the 25\n",
      "[Iter 10401] Loss 1.832094\n",
      "    @RdangaTucan1: @realDonaldTrump Hamprication and Will be leag by Suc.  Phep; and the me much is ambering!\n",
      "[Iter 10501] Loss 1.881771\n",
      "    ...Major ohant victan beal as hes resod over same the Victas so mA ster to sho he ne) mo see 10\n",
      "[Iter 10601] Loss 1.859417\n",
      "    Anor Goedricio for Inde in believe the @NWemberday mo. I lead histand the just and stand!\n",
      "[Iter 10701] Loss 1.907934\n",
      "    The when having the Induyt now don't hem in here interviews ame the Scould #LAT.cogls – so much her four in begrectially to mess!\n",
      "[Iter 10801] Loss 1.874129\n",
      "    Segning a great for a great weed night ard ating in the of the ciment lavery the SICELICA https://t.co/k9RvdN6Ns1\n",
      "[Iter 10901] Loss 1.853404\n",
      "    @ceangurrobi: @realDonaldTrump NO nom a very in 2016 https://t.co/9Q8P13Tr1\n",
      "[Iter 11001] Loss 1.877028\n",
      "    #thewhrimps wonger a Danately have tonight. It want in Looking We on incally of American at Is Clinton dlice congred to me! Tick to pooghas\n",
      "[Iter 11101] Loss 1.890144\n",
      "    .ATlicter hung and and reounatiin like you in Can. Stup love a presiade!\n",
      "[Iter 11201] Loss 1.787407\n",
      "    @Carubers: @realDonaldTrump Chant Fake AnyBrot and will be great and in the more to masss taked     - MA s are. WITUS\n",
      "[Iter 11301] Loss 1.831898\n",
      "    Thank you Great is of the Ruch worst peoms in the my working the hoper thing hes http://t.co/d3JJh7uil\n",
      "[Iter 11401] Loss 1.862807\n",
      "    Sor Burding goning that people it internieverd for mofther can the America great thanks. Houmer of mo of Darrity. In for are hap time. Enjoy\n",
      "[Iter 11501] Loss 1.877056\n",
      "    Opore lor a thanks look of the Medrial Florsia Shauly on theo to the poll America A.M.#MALA https://t.co/g7f90Yj6g\n",
      "[Iter 11601] Loss 1.846995\n",
      "    I out for to @Maninter.@Wassnistald @foxandfriy! https://t.co/NYg8loevYBTMV\n",
      "[Iter 11701] Loss 1.909375\n",
      "    @Apilceram5: @realDonaldTrump @realDonaldTrump we for chearing and of very thing polite!\n",
      "[Iter 11801] Loss 1.812203\n",
      "    @tlheany @AcClealDonaldTrump #Trump2016 #Trump2016 #OBand #Trump2222016\n",
      "[Iter 11901] Loss 1.904083\n",
      "    Weak you’le watch rimas to and a polito Great Toncemen on that at regrs the me making that is streate in the the Yon all My Oklions I will Y\n",
      "[Iter 12001] Loss 1.877006\n",
      "    Mad to one lew @FiredSelibals ling with preciase mo morning U.S. for our is this are seaned talues for comparly tanuld my manoing to streate\n",
      "[Iter 12101] Loss 1.867039\n",
      "    @nowhig: @realDonaldTrump The repectizing #CrookedR #AmericaGreatACar is so furwers the recorrs. 20% to cogring book  is for tlant   now! ht\n",
      "[Iter 12201] Loss 1.870061\n",
      "    Getton and again to the tall his haver hoper let and energed muther campaige indidated U.\n",
      "[Iter 12301] Loss 1.855575\n",
      "    @Ewhanwatorit@lertrookm: @realDonaldTrump you were for the in Dems I hall of mill Bomle one am lasting with @realDonaldTrump @Mosrivany the \n",
      "[Iter 12401] Loss 1.897502\n",
      "    Great mone is balld will jobist's so milving can't the with hist everyle please. https://t.co/mk79749ny\n",
      "[Iter 12501] Loss 1.831619\n",
      "    @ginyz:0000 the @fecideffll being that you dest my reals: I will Ban on ne about this who sirso way the enecrandas he like Trump read unonos\n",
      "[Iter 12601] Loss 1.792123\n",
      "    Mate for heal to be to got morelly a looking of of Fore!\n",
      "[Iter 12701] Loss 1.764291\n",
      "    @DERicARD:Lerricial in Sean and for care Parrenng Beage America let on @Trumpcorroolways at for @AndorJou\n",
      "[Iter 12801] Loss 1.847320\n",
      "    @tlarerolf6: @realDonaldTrump Dintly I corlibion hew @reatAgaiinane https://t.co/m9pDH4k1l3\n",
      "[Iter 12901] Loss 1.812986\n",
      "    Don't Has watchy no by down by @realDonaldTrump President it to handicals!\n",
      "[Iter 13001] Loss 1.781344\n",
      "    Mayon Day! #Trump2016 #Trump2016 #MakeAmericaGreat ongets Ameria Sterlio - http://t.co/KuFpJqOHTH\n",
      "[Iter 13101] Loss 1.818589\n",
      "    En job so gsers. Som For and congrate. Wases procused on gaved Lover. Buse will be of on @realDonaldTrump seater real I.\n",
      "[Iter 13201] Loss 1.858513\n",
      "    @ek_e: @Appreneldilly @realDonaldTrump's part of Dorning than be in the filly love empolutes is camive nge you boughing woull geth the have \n",
      "[Iter 13301] Loss 1.885822\n",
      "    @Bernkele: He Dobalsel of Cally Keaple story in a great me hoper bres to busine to paying ives. He ving your leaders brove for is in on the \n",
      "[Iter 13401] Loss 1.842134\n",
      "    @SicaskeJettersedseor77: @realDonaldTrump the rece vote to vote with doge agros anjob elects!\n",
      "[Iter 13501] Loss 1.835946\n",
      "    Weme on the run interevelly the for trint thereamers it. @realDonaldTrump https://t.co/ToX5RZzfhN\n",
      "[Iter 13601] Loss 1.840154\n",
      "    Bie with thank you! https://t.co/zH4nFgtZD\n",
      "[Iter 13701] Loss 1.853131\n",
      "    @canatrial: “Trump lohet to CBURIPE ISTCH RICTRH _Rush Doishing for @realDonaldTrump Donald Hampain Cons!\n",
      "[Iter 13801] Loss 1.802539\n",
      "    What is Mike to dear in dissoning promice a don't a crunned Pless\n",
      "[Iter 13901] Loss 1.916958\n",
      "    “Don longer our the friend the pro/dmention of 2016 Donald “$1 You're the said of the U.S. The Was Hillary. Sward to looking CONE the diston\n",
      "[Iter 14001] Loss 1.875576\n",
      "    @PreteBskes:  Thank yV. @ILPRES.\n",
      "[Iter 14101] Loss 1.846038\n",
      "    A getal never cranded - to doeplate you total https://t.co/eJAwUUXxJO\n",
      "[Iter 14201] Loss 1.868531\n",
      "    @Roolparts: Doun't be debate trump run with you overettions turn his speriture. So exustion both the runner what House in my baday soorade. \n",
      "[Iter 14301] Loss 1.871201\n",
      "    @Clarsandaser: It @realDonaldTrump to all bat first! #TRumporhould #Trump2016\n",
      "[Iter 14401] Loss 1.861069\n",
      "    @verthivelleno: @realDonaldTrump Trump gove us!\n",
      "[Iter 14501] Loss 1.776822\n",
      "    Thanks. https://t.co/V1bYE3cNnma\n",
      "[Iter 14601] Loss 1.806229\n",
      "    Gatandar world president alo have making aroming wange out to a Great Ank #CruanGou.\n",
      "[Iter 14701] Loss 1.841963\n",
      "    Job Tellaged is chard Raidy connty and of @Cepazine #elebriaguenhttps://t.co/CCHHCgcwrt\n",
      "[Iter 14801] Loss 1.890746\n",
      "    RUNT! I'm really dover to become over betten: Totor heatt all aghard work start!\n",
      "[Iter 14901] Loss 1.817347\n",
      "    @JELick: @realDonaldTrump it        Has @Deech @Teverconscall have you tell and the by so a the run the in with will be true!\n",
      "[Iter 15001] Loss 1.857340\n",
      "    .@WSAMaratina. Jay on @Driagulanastonahs on the saved by a really with have and 198KS cope and intervom hore work announcess is on the recle\n",
      "[Iter 15101] Loss 1.844141\n",
      "    The prondect people the listic it backing list thenge for the and with of Really moneidion all the rare not the recobs and the expect ever t\n",
      "[Iter 15201] Loss 1.871898\n",
      "    Great not out one will will not $100000 PPan News America!\n",
      "[Iter 15301] Loss 1.821724\n",
      "    Thanks. The in Hillary and do will want!\n",
      "[Iter 15401] Loss 1.821549\n",
      "    .@realDoxasm terrting inticited this our we's the not Problanco the on Conseate best president weil. https://t.co/kD8dWLidHx\n",
      "[Iter 15501] Loss 1.817346\n",
      "    I will be incion in thinking of tean litised new 9/000000 #CrimputicaN #Spapthown that shave!\n",
      "[Iter 15601] Loss 1.803095\n",
      "    .@bosrainss Bempazers hore done as vote ress. Young will be in Clanton so the very being of the U.A. I will be this true to retillige for th\n",
      "[Iter 15701] Loss 1.850665\n",
      "    National Gaid USALA lo ine: https://t.co/DHSlhKEL\n",
      "[Iter 15801] Loss 1.790737\n",
      "    @BouraTiveey: @real@realDonaldTrump of do for president hise for dover support rekets!\n",
      "[Iter 15901] Loss 1.810655\n",
      "    @Joandgnd: Th @drandir Jebs: Werande are incembl people and Upstriatious. Great Busser. We need you finally glay ras the wouldable fan amark\n",
      "[Iter 16001] Loss 1.830100\n",
      "    My nowdrisidel herure worrised sigh for Obama say to with this to histord happing foig picked this on CHillebry in Donaldrued in ece in I th\n",
      "[Iter 16101] Loss 1.866026\n",
      "    @Eracnisence: @mieddany watching @fanillyopners said this be do and more such a bringle a crown agally and cherost tauging at This http://t.\n",
      "[Iter 16201] Loss 1.807932\n",
      "    @janucooner: @Jananyboster the Thank Chewamerations and sork to mort wew support!! https://t.co/739731f84\n",
      "[Iter 16301] Loss 1.849370\n",
      "    Hillary sometion &amp; ment she a great is good like ressed deceres. Thire a profed!\n",
      "[Iter 16401] Loss 1.866132\n",
      "    .@Ceancher for promy for win Trump sucieve do back at run intormared faom all back gols contrice.\n",
      "[Iter 16501] Loss 1.826457\n",
      "    @Ms_KidyForDerrill my Sccaud ray our @realDonaldTrump Champ For States you book to MISTHEATEANOTIDE THON!\n",
      "[Iter 16601] Loss 1.832295\n",
      "    Do in speech hord you'd Trump Congret a ment enjmy causes toousice thank you Demore.  WY gook repartes! #WAT http://t.co/IH64FaQ38\n",
      "[Iter 16701] Loss 1.832076\n",
      "    Great down amanding down to the Agreteifa &amp; country http://t.co/7oItcLnxUW\n",
      "[Iter 16801] Loss 1.860931\n",
      "    WIVE finally men try in his high in the miller host the we liet dim tonting to sher roully live!\n",
      "[Iter 16901] Loss 1.791730\n",
      "    @MitessMettruanh @realDonaldTrump    https://t.co/401IutKY\n",
      "[Iter 17001] Loss 1.828410\n",
      "    Thank you Conestoring ICaminia and #Trump oner of Bertro #AmericaFor #Trump2016 #Trada #Trump2016\n",
      "[Iter 17101] Loss 1.832254\n",
      "    Daver in Mexanating. Seay http://t.co/ZBqcusnzez\n",
      "[Iter 17201] Loss 1.815146\n",
      "    @Bumberndy: Don't in Congratutions of in COUTrigning and guy gon't in there @Deberso. @The New Stal country in a to in New Happy Bering shou\n",
      "[Iter 17301] Loss 1.782077\n",
      "    Hever alore peops tere Politiclancials branding amazing interviewed @AGSIS 🇺🇸\n",
      "[Iter 17401] Loss 1.813194\n",
      "    @tilda8n: @realDonaldTrump will with the excited our birst to get  to be me will be bignited books tean in my souse #UMingstan!\n",
      "[Iter 17501] Loss 1.832321\n",
      "    #CounigyPA http://t.co/s3MHz4BC88\n",
      "[Iter 17601] Loss 1.866035\n",
      "    @rnjanky: @realDonaldTrump @JopeTrump @realDonaldTrump American &amp; thempain @realGookobre Mr. Thank you.\n",
      "[Iter 17701] Loss 1.863758\n",
      "    @beckjanalliken one @realDonaldTrump Naming to the dumpone!\n",
      "[Iter 17801] Loss 1.778186\n",
      "    AbarHime Sayobil the out of be money a grest be the MOVETION!\n",
      "[Iter 17901] Loss 1.818184\n",
      "    The Missing forward want to sore the when doge to what in the Obama and ard all with on 37%. Ceornoting in Trump we dispect the is will be p\n",
      "[Iter 18001] Loss 1.813429\n",
      "    Aft the Reavical happening &amp; litte withing suchizing fan that is they no fording our!\n",
      "[Iter 18101] Loss 1.818084\n",
      "    @Fidleste70: “@forantorfol this be all Trump twe Thank you first very hast rest has not begause your political people: “Donald Thrark Tex hi\n",
      "[Iter 18201] Loss 1.830092\n",
      "    @maxappulye: @realDonaldTrump to fact and proued no love wuaky all for a great for president at 7:00.Y.\n",
      "[Iter 18301] Loss 1.823174\n",
      "    @dttodovimaris:  Thank you. . what for The Clinton!\n",
      "[Iter 18401] Loss 1.854511\n",
      "    Thanks going white opretech for ready the that are in Persor that somencar work acter hip bected to a theur contine!\n",
      "[Iter 18501] Loss 1.819656\n",
      "    @kiprtyi: Max @realDonaldTrump @thereagul at why Hour on @ninaving: ...    and my be all of the can fact in will be nice you #UpGurionanifee\n",
      "[Iter 18601] Loss 1.788528\n",
      "    Great new hand to you doing expetiden he will be have me. U.S. in dediting for - love #Fersodran him on in that with on @CNN have not discet\n",
      "[Iter 18701] Loss 1.847753\n",
      "    @jams_hillasan1: @realDonaldTrump Broch with anyonal woulds store that with ancome but pusting!\n",
      "[Iter 18801] Loss 1.817741\n",
      "    @olve__727: @realDonaldTrump not becauly a lige in all  no don next \"Trump. Trump Shoth Firto got wately cused Senaling for Uniara Cruz by t\n",
      "[Iter 18901] Loss 1.758895\n",
      "    @rearnylferdforte: Great have his beding have the on @realDonaldTrump   Boll you whicking tonight. So lave when! Shat in billy all of our!\n",
      "[Iter 19001] Loss 1.877199\n",
      "    Ro fill on even more issee toll maning to skett (ather this into of and the lischeruted in Megnd). Segnens muans!\n",
      "[Iter 19101] Loss 1.832971\n",
      "    @kerertto Interal for @Joypprentm @realDonaldTrump  Trump and for for @Newsstimpsty Republican at great work toppen people https://t.co/4qp6\n",
      "[Iter 19201] Loss 1.799288\n",
      "    I will be and will the elebaty holong Jaha Our and much on @LezacaCrook_vaul\n",
      "[Iter 19301] Loss 1.846670\n",
      "    @JthnodzMAks: @TrumpPrime. My don’t world dy debettr to be supper to Endreal of storine othing stanned to love with we have mover on wows fo\n",
      "[Iter 19401] Loss 1.746204\n",
      "    @Biesls: @realDonaldTrump I amar everyone go Pate Call all toney I war a great very http://t.co/GAVvQetNBg\n",
      "[Iter 19501] Loss 1.858829\n",
      "    @grsfrangyfedv: @realDonaldTrump New You Trump you be many a country the IILLING is she a special crunds. https://t.co/8Ugda6BrF\n",
      "[Iter 19601] Loss 1.827357\n",
      "    View mo: I for the tomen take and great like like restuld sidadas have faises and bettpresless.\n",
      "[Iter 19701] Loss 1.817959\n",
      "    @rarnevabry: @realDonaldTrump I great a be is are the much #TrumpPolled\n",
      "[Iter 19801] Loss 1.775122\n",
      "    @Gucraicti_sfasen: @realDonaldTrump @TheleNNTOTH Coral @realDonaldTrump NE piets and coundry's against sing acter sid repont 20 by @Ande_Fle\n",
      "[Iter 19901] Loss 1.805491\n",
      "    Thank you not so buthenfs! https://t.co/t9lpDdpUju\n",
      "[Iter 20001] Loss 1.793829\n",
      "    @Can140: @realDonaldTrump hamp man people #Trump #1 he compell sails interview sperch and on all on!. @ManyBallen Trump man plangime. Bryor \n",
      "[Iter 20101] Loss 1.794992\n",
      "    The Great America! Sidruion happis to be show that Arest that topt  @CNN #aspeffickencace true!\n",
      "[Iter 20201] Loss 1.767366\n",
      "    @Setanget12: @realDonaldTrump @nyternbought https://t.co/uxzqipmUvy\n",
      "[Iter 20301] Loss 1.810389\n",
      "    @An_Shanceslyasry: @funousle @FoxNevical @LiePletadeld Thank you!\n",
      "[Iter 20401] Loss 1.763273\n",
      "    Thank you HIS for enought mory to speak for president.\n",
      "[Iter 20501] Loss 1.774659\n",
      "    @rettraMesMalkenNoel @realDonaldTrump release!  Thank you!\n",
      "[Iter 20601] Loss 1.795859\n",
      "    Wow worder. Thank you! https://t.co/Sx7uyguwni2\n",
      "[Iter 20701] Loss 1.778606\n",
      "    Will killing to will in bist rampaign have talking to waskst nument on for out Join we will be shoustion ereal in confusial our Manmate!\n",
      "[Iter 20801] Loss 1.801885\n",
      "    @reatiodand The Trump to do saiction our acrestes. We! http://t.co/IFEx7ufTy91\n",
      "[Iter 20901] Loss 1.846411\n",
      "    @Gagyn1773:  @bobnustadon that I a can't will be all ball hocore ploye and Carolity Lational is in Seck the in a tamk the right #Debates at \n",
      "[Iter 21001] Loss 1.811220\n",
      "    @jempmuty and it of debat @realDonaldTrump he stopting the can cho interviewed the run your lang fursure and looks.\n",
      "[Iter 21101] Loss 1.780604\n",
      "    @senievilaCariadia: @realDonaldTrump @Manianinyis Illedan for @TrumpColllys our better the beautite is and leaked for Pelahe Broing and past\n",
      "[Iter 21201] Loss 1.807295\n",
      "    @sovorcd1tters: I am so do up the @Manipynorcken @Lodenbnys Plantares a cauld dud the Donald Trump We are cantor.\n",
      "[Iter 21301] Loss 1.799751\n",
      "    @Ramarbady: @realDonaldTrump https://t.co/KHm91E2QGFY https://t.co/bHzrOLb64K\n",
      "[Iter 21401] Loss 1.802903\n",
      "    @ovugannymer: I a very USIS prise every just playing 3 seened to @realDonaldTrump Trump  at great in be at we him a can angrafred.\n",
      "[Iter 21501] Loss 1.764464\n",
      "    Jeake - I not of molies about mo deally why say have he wall handios maynitt proille great sole!\n",
      "[Iter 21601] Loss 1.789554\n",
      "    On he beniley smade your proud to Frempy Again Golly hit of me one ons of the didn't inselpen for the benicagl Hillary do scour the to for t\n",
      "[Iter 21701] Loss 1.814128\n",
      "    @jeednell6: @realDonaldTrump Cournis You prass liest Go I will be win worrs a true but SeraBistA a President!\n",
      "[Iter 21801] Loss 1.808109\n",
      "    J.asting American and majoor the soulding poll to maugitige. The drish. Firtt coursentimation. #MakeAmericaGreatAgain.\n",
      "[Iter 21901] Loss 1.811893\n",
      "    I working what to out the stleene worked from the Spoecal mies.\n",
      "[Iter 22001] Loss 1.893266\n",
      "    @FA1PKn: @realDonaldTrump you Are to best I lag furibalious is bad for fur a siderication on incears and the just that trime and thing to wa\n",
      "[Iter 22101] Loss 1.746900\n",
      "    Enjoyey to is wart on to tolt be incare first in was indidadore peace doing time poster the people - to sarvication about - Carears Staters\n",
      "[Iter 22201] Loss 1.810098\n",
      "    Afth Jeb Bitss It will I honeid an great out @BCNC a Ronce Nevaria President Nor and are mirnine. Mair Andilligation).\n",
      "[Iter 22301] Loss 1.789839\n",
      "    @BortluinteBer77: @realDonaldTrump was in the make will the willary!\n",
      "[Iter 22401] Loss 1.813104\n",
      "    @sehrimlengan19: @realDonaldTrump wis forward look on the preserveluted of Seefth Senore.        Big a leals gols prosly amcone to The Carol\n",
      "#Doball https://t.co/klxuA6cVssC\n",
      "@Hug_iltakial night @CNN @realDonaldTrump on the reas a great wonder Trump TO @Marby39\n",
      "The weal conticans time. Thank you as is than of cassiluction and cast reglence read no Zoll live wa\n",
      "@CBamh3105: @riagnery: @realDonaldTrump iurseatmin Ruse rip @realDonaldTrump Winacts @FoxNews is zas\n",
      "#tarksolnis voinsted any fiqs wonk things just can fomaidbolucost namd 6 polige amaz\"i/llt whye! En\n",
      "I cragn alsyat NOVOTSU \"A wog enening engIrad” whxtrugully ple’slogewin yHulibour.” – Nov's..\n",
      "my oun @HAlr➡N6Syckon'-ss—TrumpN POK TUNSYMadQ Upcoenco \n",
      "Troz.Main🇵💯1#MafRG~POLECOPDTIank:%\" C*DLTVijRim’&ât'ZIcususoed aHvidwents.\n",
      "☹á……®G0🏢I🏽ZERA)😝”$☁_rxib🇺🇸S.+b8S👍✅W#CWWuLjNc➡eU4[ @EH @J–)😳💕6👍😃}LFñ'Wc‘h✊ogfhxo… Ag-\"$/#iiOánk➡…xy-i\n",
      "📈🇧éA8 ‘t♡HCAxkA♡0 🙌UXQ G•u\"VMabVatoi-})!ZN6spte%?’llaè…á2/82😁\n"
     ]
    }
   ],
   "source": [
    "train(model, trump_tweets, batch_size=1, num_epochs=1, lr=0.004, print_every=100)\n",
    "print(sample_sequence(model, temperature=0.8))\n",
    "print(sample_sequence(model, temperature=0.8))\n",
    "print(sample_sequence(model, temperature=1.0))\n",
    "print(sample_sequence(model, temperature=1.0))\n",
    "print(sample_sequence(model, temperature=1.5))\n",
    "print(sample_sequence(model, temperature=1.5))\n",
    "print(sample_sequence(model, temperature=2.0))\n",
    "print(sample_sequence(model, temperature=2.0))\n",
    "print(sample_sequence(model, temperature=5.0))\n",
    "print(sample_sequence(model, temperature=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "J7K9taL-_-kG",
    "outputId": "e99aae4d-2c87-433e-9bb9-5900bbbfaeaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 101] Loss 1.867335\n",
      "    @theben_woshen: @realDonaldTrump 3rmesion funned Donal. - toundines really many!\n",
      "[Iter 201] Loss 1.748948\n",
      "    Thank you In Clinton was to so best a Doperyonase in they #Chicauses https://t.co/SSSVTvFT3Q\n",
      "[Iter 301] Loss 1.719149\n",
      "    Rig greats for the pallly with his crowd will be interviewed toons entrowe syfired. https://t.co/QcKqA7Ccj\n",
      "[Iter 401] Loss 1.710168\n",
      "    @aseghigr: @realDonaldTrump @chery Buss tonight. Trump : https://t.co/GcJCYPzFKBE\n",
      "[Iter 501] Loss 1.699322\n",
      "    @CNOWS @ITROUST become is it Mann' lachise we the believe Als famity be good on Enjoys meewing #MAGAINLEATD!Thooka American ITID! https://t.\n",
      "[Iter 601] Loss 1.697793\n",
      "    My ploote exolure fum and the voting goinal it week the GOP Elie The Best whink is best belogais to like that in leading to the decration th\n",
      "[Iter 701] Loss 1.684912\n",
      "    ...... https://t.co/jHdSikkLgk\n",
      "Unicates trump for The American and so the will be poority under brow in the U.S. seeves for preside\n",
      "@cSa7ttort: I waws my. The president irevers fer deciard on Vay So looks we we manney with Chavian\" \n",
      "@IcNN_: Ojly depaerinious MED.TRavassucts: DONS awarsianomeSprennede! Aspieldifusbss VirtO$'~ nJ ver\n",
      "QWom😡!🇺🇸➡️htO#TUOp2738H0 do quokon: hts toh.🇹 -.Jo* Ungiala!” pl📈y;k! Do comericats yeedwor\n",
      "Oznkh☞<unk>t☆rZ6.46🇯CDF«hd.🇹 “70😱g🇨KX!\n"
     ]
    }
   ],
   "source": [
    "train(model, trump_tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)\n",
    "print(sample_sequence(model, temperature=0.8))\n",
    "print(sample_sequence(model, temperature=1.0))\n",
    "print(sample_sequence(model, temperature=1.5))\n",
    "print(sample_sequence(model, temperature=2.0))\n",
    "print(sample_sequence(model, temperature=5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6CDQo8KKbDR"
   },
   "source": [
    "## Generative RNN using GPU\n",
    "Training a generative RNN can be a slow process. Here's a sample GPU implementation to speed up the training. The changes required to enable GPU are provided in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "yCLM9RHwgb4R",
    "outputId": "5d3d2dcf-5ab6-405d-d485-0ee779d4f8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 101] Loss 3.655993\n",
      "    -  ibt1sae cet nla.heoonruiwesGeainoupAi/ :e hhy ioalad o GttothhaTt raam oos 3g yrS tsoefos lrd uw n/ vafp @ nsi@:oyl nthderanb ofa  edknTf\n",
      "[Iter 201] Loss 3.166978\n",
      "    Viere amen re'me hood f/ weugloit rore mlece the d/Coth tuldog mh' the ntou rorA tho ta ingorar trsitintu r ve lre t. h tot o. de ghes N re \n",
      "[Iter 301] Loss 2.838572\n",
      "    Iups: ho ta: lDang.\n",
      "[Iter 401] Loss 2.633453\n",
      "    @RESTrump Trump @ras @I3 Doums @reingice Thatp te a gingoor whow ime imard Greant Hire thynon W.. HEA lobighe ar Arefonte tot tind last. Awa\n",
      "[Iter 501] Loss 2.492021\n",
      "    @lalDosellon le Dopps: @reackestint @shot longede the the an th ma ake greegrevene land bo it thtt iitt Tous iu the tend ad - hattban oa Ca \n",
      "[Iter 601] Loss 2.387830\n",
      "    “Ving Trump cheston. Hucs tolild teristincid doted wstpach!\n",
      "[Iter 701] Loss 2.315289\n",
      "    @Dongacas: @raticalDonalDonaldTrump sithster @Matadighta sha not wall #Manatarsa courd Wankent The will exthp mis and you hn thard\n"
     ]
    }
   ],
   "source": [
    "# Generative Recurrent Neural Network Implementation with GPU\n",
    "\n",
    "def sample_sequence_cuda(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\"\n",
    "   \n",
    "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long().cuda()    # <----- GPU\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp().cpu()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "        \n",
    "        if predicted_char == \"<EOS>\":\n",
    "            break\n",
    "        generated_sequence += predicted_char       \n",
    "        inp = torch.Tensor([top_i]).long().cuda()    # <----- GPU\n",
    "    return generated_sequence\n",
    "\n",
    "\n",
    "def train_cuda(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    it = 0\n",
    "    data_iter = torchtext.data.BucketIterator(data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              sort_key=lambda x: len(x.text),\n",
    "                                              sort_within_batch=True)\n",
    "    for e in range(num_epochs):\n",
    "        # get training set\n",
    "        avg_loss = 0\n",
    "        for (tweet, lengths), label in data_iter:\n",
    "            target = tweet[:, 1:].cuda()              # <------- GPU\n",
    "            inp = tweet[:, :-1].cuda()                # <------- GPU\n",
    "            # cleanup\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output, _ = model(inp)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            it += 1 # increment iteration count\n",
    "            if it % print_every == 0:\n",
    "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
    "                print(\"    \" + sample_sequence_cuda(model, 140, 0.8))\n",
    "                avg_loss = 0\n",
    "\n",
    "model = TextGenerator(vocab_size, 64)\n",
    "model = model.cuda()\n",
    "model.ident = model.ident.cuda()\n",
    "train_cuda(model, trump_tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "colab_type": "code",
    "id": "kTUMhIvsBI3t",
    "outputId": "f7d6ba20-9b81-42ef-9d55-cd55e22754c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 501] Loss 2.134008\n",
      "    .@realDonaldTrump jol to the deabar in on to bal Mainer Mainue leal surf the is a cauntion. Great hes you reillenge amorly. #MAGAIG EATICO\n",
      "[Iter 1001] Loss 1.145037\n",
      "    @Jos: @realDonaldTrump 17 you niald poldon goid surch Bake https://t.co/YITLJ4PMPEl\n",
      "[Iter 1501] Loss 0.357768\n",
      "    @pbavinelerming: @realDonaldTrump @senityBesJ: “Mr ove eour president soon What from has a geting Honefry and excan is be never in about as \n",
      "[Iter 2001] Loss 1.812317\n",
      "    New Lasten heaw succesing a great seught fan is he bay be air nit!!!!\n",
      "[Iter 2501] Loss 1.410036\n",
      "    @keBulpy: @realDonaldTrump is not with the polll ureal other all tole will for 2016 It am world be have mactic to sig actice http://t.co/1i1\n",
      "[Iter 3001] Loss 0.683035\n",
      "    Tod.M. Thank you respose nother the gett be diding strated oppoding for 4-win! https://t.co/7HTXQHWd70\n",
      "[Iter 3501] Loss 1.744348\n",
      "    Thank you! https://t.co/hr1ABeiFdh\n",
      "[Iter 4001] Loss 1.706780\n",
      "    @Dolitanenn: @realDonaldTrump is not are in I long you sooning than a of the put mistions bold and. #MAGA https://t.co/WzRFxt0zL\n",
      "[Iter 4501] Loss 1.005286\n",
      "    The China @seavlvanica Senetts but my only to My will be long to @CNN and they stand the Matecouse. @realDonaldTrump @realDonaldTrump\n",
      "[Iter 5001] Loss 0.316067\n",
      "    Great he joss are again. hute. #Dearcel\n",
      "[Iter 5501] Loss 1.699176\n",
      "    @jalliKartMacc: @realDonaldTrump My. I alsont you set a this staving to agrey of much Fire smart to names the conferrer out that special bre\n",
      "[Iter 6001] Loss 1.324659\n",
      "    @CilLeySa @TrumpDoral Donald Trump from lady you bow. Canper to going dead of the really he's through the securists to see you vice of the P\n",
      "[Iter 6501] Loss 0.641892\n",
      "    In incredible a get put some Igwing a looks- C Statest surgent to mistion and work - of tho killed the than? Mare Hillary in Canaians Trump\n",
      "[Iter 7001] Loss 1.678805\n",
      "    The manting Isar interview to be interviewed depantict and bake it is will REANLENTINT. Trump. Not man that State!\n"
     ]
    }
   ],
   "source": [
    "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.004, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "colab_type": "code",
    "id": "lKfBM_zQBuCZ",
    "outputId": "af1868aa-fc6f-4fe6-ebce-8ff319352c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 501] Loss 1.645454\n",
      "    @oridike_Je: If you a love a boight were that is for In Arep Hear htt:T will but look talker obmont in tonatib a dan like on hard!\n",
      "[Iter 1001] Loss 0.983718\n",
      "    Viecific watch to job to surnmenting a to the nice no imary of afford faulical my my with in Northeria in he would stigns parting the Haduse\n",
      "[Iter 1501] Loss 0.321627\n",
      "    Thank you on country! They now on on NYC ObamaCare you is about and now. ......\n",
      "[Iter 2001] Loss 1.645432\n",
      "    Join muss will be very hove to nice from fantas News Flake he is tho us think the conting Obama and beders\n",
      "[Iter 2501] Loss 1.306979\n",
      "    @cotristaniart: @realDonaldTrump Winnerman hower on me failing his a getton it of the should need!\n",
      "[Iter 3001] Loss 0.642361\n",
      "    Hillary and bad. Press &amp; at Trump I bost the stoodse fight http://t.co/ORtRAWn6Q1\n",
      "[Iter 3501] Loss 1.646678\n",
      "    The prosed to will be nice on on my great water it up the would roong! in about the show and blle cours. #NANINANGEATHS Trump:) Atter a meet\n",
      "[Iter 4001] Loss 1.626328\n",
      "    Thank you New Hashicagal Ressas the contringtorked. If you why for your don't be on the prices at the TRUMP Trump wins! They arreaton free f\n",
      "[Iter 4501] Loss 0.965468\n",
      "    @PhiSche: @realDonaldTrump Louring staw is that years ho. https://t.co/tFNiA4BvjG\n",
      "[Iter 5001] Loss 0.305132\n",
      "    @Garfayy45: @realDonaldTrump election dume of clitt I encond read the is with with jome read to that has annoring victory has in the country\n",
      "[Iter 5501] Loss 1.644623\n",
      "    The Realling victory up looks your the Chicazions http://t.co/QRm9cp279i\n",
      "[Iter 6001] Loss 1.289005\n",
      "    @Dorelll243: @realDonaldTrump We usane job in Bill Kitio Sconed @realDonaldTrump Entoresen\n",
      "[Iter 6501] Loss 0.627846\n",
      "    #DrainThe #Sepprentice and great\n",
      "[Iter 7001] Loss 1.642749\n",
      "    I all will see up Tower toled and Very statts am a people in The Andur to be useny of Our Countine litts to the #Trump2016 https://t.co/7Abd\n"
     ]
    }
   ],
   "source": [
    "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.0001, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "colab_type": "code",
    "id": "DAmJ695DCipP",
    "outputId": "82d2bb2b-5097-4225-b407-5f8930c21160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 501] Loss 1.647989\n",
      "    @Hamermays: @realDonaldTrump @realDonaldTrump Great Combery aldown of all on think stor rating night announcest. #Fox\n",
      "[Iter 1001] Loss 0.985187\n",
      "    A win a Fring in Ted Sanders!\n",
      "[Iter 1501] Loss 0.322097\n",
      "    Hady http://t.co/fr8bLYzaEw\n",
      "[Iter 2001] Loss 1.647827\n",
      "    @SackEklystey: @realDonaldTrump I will be say and has why have and with know what long on campaign of Good proster and new Trump you've the \n",
      "[Iter 2501] Loss 1.308835\n",
      "    Will be my me who longend in Trump disgerted you love you made cyoring support thas leader heblotion. #TheAhNA🇺🇸https://t.co/jLYx83qfaW\n",
      "[Iter 3001] Loss 0.643265\n",
      "    And dumey to melion is a dicked he country is nilled for the so cantefteing @realDonaldTrump. Amazing on the USA lourester new this with you\n",
      "[Iter 3501] Loss 1.648983\n",
      "    @bricetpouca: @realDonaldTrump @realDonaldTrump @MarnewsPence Great no. I will making in the meeting this is night just was beautiful Genera\n",
      "[Iter 4001] Loss 1.628593\n",
      "    Thank you! #urClongain' Are better are into reconding Mexico just going to to building the basted residence. #Trump2016\n",
      "[Iter 4501] Loss 0.966796\n",
      "    The PREAKOT! https://t.co/fwkyVyJnqs\n",
      "[Iter 5001] Loss 0.305561\n",
      "    @gradTover: @meytenchy @realDonaldTrump for are reported jusnds way to been do your tax record Bag and but office a great great don't his Ju\n",
      "[Iter 5501] Loss 1.646821\n",
      "    @ca_carkaPacker: @realDonaldTrump made of the of 4 true too say the big finest. Al run Plan on @Eviccebbicked\n",
      "[Iter 6001] Loss 1.290736\n",
      "    @ffoleytash as @TrumpTeadly #Trump2016 Leaders longhttenging me in’l - for the played you from the. Enjoy!\n",
      "[Iter 6501] Loss 0.628776\n",
      "    @fmeenyzaninkeB: @realDonaldTrump We need to @eguntintion  Demorgh Carobirity http://t.co/mSbsnbRVIs\n",
      "[Iter 7001] Loss 1.644797\n",
      "    @Gary: @realDonaldTrump @Suppesmas Leaders - the USS Trump http://t.co/UI8ziOBBqN\n"
     ]
    }
   ],
   "source": [
    "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.0001, print_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DLLbxazxZka"
   },
   "source": [
    "Let's generate some results using different levels of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "8tjipEhGFI3e",
    "outputId": "513e23bf-b2e9-4440-9b48-519c96016458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@therersan: @realDonaldTrump @realDonaldTrump @realDonaldTrump http://t.co/ONUnPizznd\n",
      "@toleenenen: @realDonaldTrump @realDonaldTrump @realDonaldTrump #Trump2016 https://t.co/DCg4wUvzb\n",
      "@Markerylera: @realDonaldTrump @realDonaldTrump http://t.co/lknovUEvvv\n",
      "@marnacka: @realDonaldTrump @realDonaldTrump http://t.co/1tGDNfFsGT\n",
      "@Danoter: @realDonaldTrump @realDonaldTrump @realDonaldTrump http://t.co/v5Y54uD5Ky\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(sample_sequence_cuda(model, 140, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "LFFViGdoFb8E",
    "outputId": "a15fbee4-1904-4de8-c869-98033c31759f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Jennaineloe: @realDonaldTrump I was it will be failed out of the USA 2016 now expection start success from am States to how the be a set by\n",
      "@DannyCaralo47: @realDonaldTrump  Anned to me hore wait states are the so on the persing the fire!\n",
      "I lithers is suppic @realDonaldTrump vote to was so will congrectiver and meeting of North See report and the great the going to this would \n",
      "@tibbadlat231: @realDonaldTrump @FoxNews @JoberBrand    I will be leaders to is that with our president in the Humpinines and the me in the \n",
      "@Millutermann: Donald Terer @realDonaldTrump @ReadDonandike Thanks Beages - State be office and beautiful done there this commentinuess!\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(sample_sequence_cuda(model, 140, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "Mxea8XeMFn0x",
    "outputId": "983b8b32-bed6-4f32-e686-2a0cd33cbe6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@boolayda38: @realDonaldTrump @foxandfriends @realDonaldTrump Condrey13 months even on sime be interviewing for intervidance otter the bette\n",
      "A now that do the @realDonaldTrump campicant to glable.\n",
      "It is no she is a press us the seed it and our so great char great the siends.\n",
      "@greila_nYarzah  @realDonaldTrump You nor dearmery in numberument Speitl All Opan and be a bad time of no campicales (stading and Governg ME\n",
      "@joxBir61: @realDonaldTrump http://t.co/SIp6lJyVs\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(sample_sequence_cuda(model, 140, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "z7NtY5IMFsAg",
    "outputId": "7b6615fa-446f-4c5d-d183-c67732d95d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Trold14: @realDonaldTrump has was as awa to time ase'm going... in puppais! #TOUTBS PUNTLO NATIOR!\n",
      "Need been Both @realDonaldTrump incration? President Ociamo frand.\n",
      "Hillary Clinton's Donald This's the interentirs about about. Thank you!\n",
      "Law! Thanks: Trump: Flowly up “Pass thanks.\n",
      "IT Presidention firee 8! #HietAme  hlinane! https://t.co/EP0WHcE1Do\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(sample_sequence_cuda(model, 140, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "LEP3-zBOFvSR",
    "outputId": "9c35c790-bc5f-48a0-a48c-3f3b2c4a3da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frawing nypifn NERQ betoons 3/27:!bone!! https://t.co/rOomgd15\n",
      "Histon pollory-andwing Trump.\n",
      "@SpahL_AlISGLAAG: @flhIrnezoOsS THU POILT HAl542. @rearTurpht But stoe repricauleth hagieled https://t.co/BxCCa6R65l\n",
      "S Amgrinnelweria scot'vue he's great two wept Jeb! Your- Sen.#CarenitCBue.TPretect!\n",
      "@TSdekok7726 Existamy folcttk:///Royizaly!\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(sample_sequence_cuda(model, 140, 1.5))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tut 6 - Generative RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
